{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"/workdir\")\n",
    "os.environ[\"GLUE3D_CACHE_DIR\"] = \"../.cache\" # <- setup environment variable to cache GLUE3D data\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from glue3d.models.loaders import load_pointllm_model\n",
    "\n",
    "model, tokenizer = load_pointllm_model(\"RunsenXu/PointLLM_7B_v1.2\") # Load qwen2 VL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glue3d.models.hf import BinaryHFGenerator, CaptioningHFGenerator\n",
    "\n",
    "\n",
    "class BinaryPointLLMAnswerGenerator(BinaryHFGenerator):\n",
    "    def __init__(self,model,tokenizer,**kwargs):\n",
    "        super().__init__(model, tokenizer, **kwargs)\n",
    "\n",
    "        from pointllm.conversation import conv_templates, SeparatorStyle\n",
    "\n",
    "        self.conv_template = conv_templates[\"vicuna_v1_1\"].copy()\n",
    "        self.stop_string = (\n",
    "            self.conv_template.sep if self.conv_template.sep_style != SeparatorStyle.TWO else self.conv_template.sep2\n",
    "        )\n",
    "        self.point_backbone_config = self.model.get_model().point_backbone_config\n",
    "\n",
    "    def prepare_data(self, data, text: str) -> str:\n",
    "        import torch\n",
    "        from pointllm.model.utils import KeywordsStoppingCriteria\n",
    "\n",
    "        point_backbone_config = self.point_backbone_config\n",
    "        point_token_len = point_backbone_config[\"point_token_len\"]\n",
    "        point_patch_token = point_backbone_config[\"default_point_patch_token\"]\n",
    "        point_start_token = point_backbone_config[\"default_point_start_token\"]\n",
    "        point_end_token = point_backbone_config[\"default_point_end_token\"]\n",
    "        mm_use_point_start_end = point_backbone_config[\"mm_use_point_start_end\"]\n",
    "\n",
    "        if mm_use_point_start_end:\n",
    "            user_text = point_start_token + point_patch_token * point_token_len + point_end_token + \"\\n\" + text\n",
    "        else:\n",
    "            user_text = point_patch_token * point_token_len + \"\\n\" + text\n",
    "\n",
    "        conv = self.conv_template.copy()\n",
    "        conv.append_message(conv.roles[0], user_text)\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "        prepared_texts = conv.get_prompt()\n",
    "\n",
    "        device, dtype = self.model.device, self.model.dtype\n",
    "        input_ids = self.tokenizer(prepared_texts, return_tensors=\"pt\").input_ids.to(device=device)\n",
    "\n",
    "        stopping_criteria = KeywordsStoppingCriteria([self.stop_string], self.tokenizer, input_ids)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"point_clouds\": torch.from_numpy(data).to(device=device, dtype=dtype).unsqueeze(0),\n",
    "            #\"stopping_criteria\":stopping_criteria,\n",
    "            **self.kwargs\n",
    "        }\n",
    "\n",
    "answer_gen = BinaryPointLLMAnswerGenerator(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glue3d.generate_answers import generate_GLUE3D_answers\n",
    "\n",
    "out_df = generate_GLUE3D_answers(\"binary_task\", \"GLUE3D-points-8K\", answer_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
